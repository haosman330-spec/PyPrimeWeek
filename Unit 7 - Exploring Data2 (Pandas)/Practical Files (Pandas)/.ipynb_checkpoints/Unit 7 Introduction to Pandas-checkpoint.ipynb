{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc Artificial Intelligence Python Primer\n",
    "# Unit 7 Jupyter Notebook \n",
    "# Introduction to Pandas\n",
    "\n",
    "\n",
    "## Goals\n",
    "This notebook has been created to familiarise you with the Pandas package. Most of the code needed to progress through this Notebook has been provided for you. However, there are several coding tasks that you will need to complete yourself by entering code yourself.\n",
    "\n",
    "The topics in this notebook include:\n",
    "* Pandas ``Series``, ``DataFrame`` and ``Index`` data structures\n",
    "* Loading (and saving) Data\n",
    "* Indexing and Selection\n",
    "* Handling missing data\n",
    "* Combining Datasets\n",
    "* Aggregation and Grouping\n",
    "* Vectorised String Operations\n",
    "* Working with Time Series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Unit 6, we focused on NumPy arrays (also known as `ndarrays`), which provide efficient storage and manipulation of dense typed arrays in Python. This unit builds on this knowledge by looking in detail at the data structures provided by the Pandas library. Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a ``DataFrame``.\n",
    "``DataFrame``s are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Pandas Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. Pandas provides a host of useful tools, methods, and functionality on top of the basic data structures, but nearly everything that follows will require an understanding of what these structures are. There are three fundamental Pandas data structures: the ``Series``, ``DataFrame``, and ``Index``.\n",
    "\n",
    "The Pandas library is available in the Anaconda installation and so you simply need to import the package to access its tools and capabilities. By convention, you'll find that most people in the data science world will import Pandas using ``pd`` as an alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pandas Series Object\n",
    "A Pandas ``Series`` is a one-dimensional array of indexed data. It can be created from a List, Dictionary or NumPy Array. A ``Series`` has no column labels - it is just a single column of a DataFrame. However, a ``Series`` does have row labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with a NumPy array, data can be accessed by the associated index via the familiar Python square-bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or an array slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Pandas ``Series`` is much more flexible than the one-dimensional NumPy array that it emulates. The essential difference between a ``Series`` object and a NumPy Array is the presence of the index. While the Numpy Array has an *implicitly defined* integer index used to access the values, the Pandas ``Series`` has an *explicitly defined* index associated with the values. This explicit index definition gives the ``Series`` object additional capabilities. For example, the index need not be an integer, but can consist of values of any desired type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the item access works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could construct a ``Series`` object directly from a Python Dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a ``Series`` will be created where the index is drawn from the sorted keys.\n",
    "From here, typical dictionary-style item access can be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population['California']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a dictionary, though, the ``Series`` also supports array-style operations such as slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population['New York':'Illinois']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pandas DataFrame Object\n",
    "\n",
    "Like the ``Series`` object discussed in the previous section, the ``DataFrame`` can be thought of either as a generalization of a NumPy array, or as a specialization of a Python dictionary. It is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table or the data.frame in R.\n",
    "\n",
    "If a ``Series`` is an analog of a one-dimensional array with flexible indices, a ``DataFrame`` is an analog of a two-dimensional array with both flexible row indices and flexible column names. To illustrate, let's construct a new ``Series`` listing the area of each of the five states discussed in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the ``area`` Series with the ``population`` Series to construct a single two-dimensional object containing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also think of a ``DataFrame`` as a specialization of a dictionary. Where a dictionary maps a key to a value, a ``DataFrame`` maps a column name to a ``Series`` of column data. For example, asking for the ``'area'`` attribute returns the ``Series`` object containing the areas we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to create a `DataFrame` from tabular data is to use a Dictionary of Lists (or NumPy Arrays). The following example uses the same information as the examples above. When using this approach we also need to explicitly define the index values using the `index` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'population':[38332521,26448193,19651127,19552860,12882135],\n",
    "      'area':[423967,695662,141297,170312,149995]}\n",
    "states = pd.DataFrame(data, index=['California','Texas','New York','Florida','Illinois'])\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pandas Index Object\n",
    "\n",
    "We have seen here that both the ``Series`` and ``DataFrame`` objects contain an explicit *index* that lets you reference and modify data. This ``Index`` object can be thought of as an *immutable array* or as an *ordered set*. As a simple example, let's construct an ``Index`` from a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.Index([2, 3, 5, 7, 11])\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index as immutable array\n",
    "\n",
    "The ``Index`` in many ways operates like an array.\n",
    "For example, we can use standard Python indexing notation to retrieve values or slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between ``Index`` objects and NumPy arrays is that indices are immutable–that is, they cannot be modified via the normal means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This immutability makes it safer to share indices between multiple ``DataFrame``s and arrays, without the potential for side effects from inadvertent index modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index as ordered set\n",
    "\n",
    "Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic.\n",
    "The ``Index`` object follows many of the conventions used by Python's built-in ``set`` data structure, so that unions, intersections, differences, and other combinations can be computed in a familiar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA = pd.Index([1, 3, 5, 7, 9])\n",
    "indB = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA & indB  # intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA | indB  # union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indA ^ indB  # symmetric difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'><u>Worksheet Exercises</u></font>\n",
    "1. Using an appropriate NumPy function, create a ``Series`` object with 40 equally space values between 0 and 10 (i.e. every 0.25)\n",
    "2. The following table shows the percentage frequency of online purchases in 2020 by men. Create a ``Series`` object to represent the data shown in the following table:\n",
    "\n",
    "| index | value |\n",
    "|---|---|\n",
    "|  once or twice | 18 |\n",
    "| three to five times | 28 |\n",
    "| six to ten times | 20 |\n",
    "| eleven times or more | 34 |\n",
    "\n",
    "3. The following Python `Dictionary` definition includes the frequency of online purchases in 2020 by women: `{'once or twice': 15, 'three to five times': 32, 'six to ten times': 18,'eleven times or more': 34}`. Use this `Dictionary` to create a second ``Series`` object \n",
    "4. Now, combine the two ``Series`` from 2. and 3. above to create a new ``DataFrame`` object.\n",
    "5. The following table shows the percentage frequency of online purchases in 2020 by age group. Create a ``DataFrame`` object to represent this data.\n",
    "\n",
    "|  | 16-24 | 25-34 | 35-44 | 45-54 | 55-64 | 65+ |\n",
    "|---|---|---|---|---|---|---|\n",
    "| once or twice | 25 | 11 | 12 | 9 | 19 | 26 |\n",
    "| three to five times | 30 | 26 | 23 | 36 | 25 | 39 |\n",
    "| six to ten times | 20 | 18 | 20 | 17 | 22 | 19 |\n",
    "| eleven times or more | 24 | 43 | 44 | 37| 34 | 15 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your exercise solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data\n",
    "We might have your data in .csv files or SQL tables. Maybe Excel files. Or .json files. The goal is the same in all cases. If we want to analyze that data using Pandas, the first step will be to read it into a data structure that’s compatible with Pandas, i.e. a ``Series`` or ``DataFrame``. Once we have completed some analysis work we might want to write the result out into a file too. In this section, we will look at how to read/write data using two of the most popular forms of data file that we are likely to encounter - CSV and JSON. However, Pandas is capable of reading HTML, Excel, SPSS, SQL and many more. Some of these will require additional libraries to be installed (see https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html for more details)\n",
    "\n",
    "### CSV Files\n",
    "A comma-separated values (CSV) file is a plaintext file with a .csv extension that holds tabular data. This is one of the most popular file formats for storing large amounts of data. Each row of the CSV file represents a single table row. The values in the same row are by default separated with commas, but we could change the separator to a semicolon, tab, space, or some other character\n",
    "\n",
    "#### Reading a CSV File\n",
    "Pandas provides the `read_csv()` function to read the contents of a .csv file into a ``DataFrame``. This function returns a new ``DataFrame`` with the data and labels stored in .csv file which we specify. This file specification can be any valid path, including URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reqd a .csv data file into memory and display the top 5 rows\n",
    "presidents = pd.read_csv('data/president_heights.csv')\n",
    "presidents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the CSV file does not include a header row, we can use the `header=None` parameter. However, when using this parameter, columns will be given a default index number starting at 0. This is not particularly helpful for further analysis work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adults = pd.read_csv('data/adults.csv', header=None)\n",
    "adults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this issue, `header=None` should always be used in conjunction with the `names` parameter. This parameter allows us to provide a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adults2 = pd.read_csv('data/adults.csv', header=None, \n",
    "                    names=['age','workclass','fnlwgt','education','education_num',\n",
    "                           'marital_status','occupation','relationship','race','sex',\n",
    "                           'capital_gain','capital_loss','hours_per_week','native_country','wages'])\n",
    "adults2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a CSV File\n",
    "Pandas provides the `to_csv()` method to write the contents of a ``DataFrame`` to .csv file. This will create a file in the current working directory with the data and column names encoded in the DataFrame. In a DataFrame the first column always contains the row labels. We can prevent these being added to the file by adding the parameter `index=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents.to_csv('presidents.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files\n",
    "A JSON file is a file that stores simple data structures and objects in JavaScript Object Notation (JSON) format, which is a standard data interchange format. It is primarily used for transmitting data between a web application and a server. JSON files are lightweight, text-based, human-readable, and can be edited using a text editor.\n",
    "\n",
    "#### Reading a JSON File\n",
    "Pandas provides the `read_json()` function to read the contents of a .json file into a ``DataFrame``. This function returns a new ``DataFrame`` with the data and labels stored in .json file which we specify. This file specification can be any valid path, including URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_json('data/country_data.json')\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a JSON File\n",
    "Pandas provides the `to_json()` method to write the contents of a ``DataFrame`` to .json file. This will create a file in the current working directory with the data and column names encoded in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_json('countries.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Indexing and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Unit 6, we looked in detail at methods and tools to access, set, and modify values in NumPy arrays including indexing, slicing, masking, advanced indexing and concatenations. Here we'll look at similar means of accessing and modifying values in Pandas ``Series`` and ``DataFrame`` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection in Series\n",
    "The following Series is used in this throughout this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series as one-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ``Series`` provides array-style item selection via the same basic mechanisms as NumPy arrays – that is, *slices* and *masking*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by explicit index - final index is included in the slice\n",
    "data['a':'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by implicit integer index - final index is excluded from the slice\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking\n",
    "data[(data > 0.3) & (data < 0.8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series as dictionary\n",
    "\n",
    "Like a dictionary, the ``Series`` object provides a mapping from a collection of keys to a collection of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Series`` objects can be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a ``Series`` by assigning to a new index value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['e'] = 1.25\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection in DataFrame\n",
    "\n",
    "Remember that a ``DataFrame`` can act like a dictionary of ``Series`` structures sharing the same index, and also like a two-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame as a dictionary\n",
    "\n",
    "Firstly, consider the ``DataFrame`` as a dictionary of related ``Series`` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.Series({'California': 423967, 'Texas': 695662,\n",
    "                  'New York': 141297, 'Florida': 170312,\n",
    "                  'Illinois': 149995})\n",
    "pop = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
    "                 'New York': 19651127, 'Florida': 19552860,\n",
    "                 'Illinois': 12882135})\n",
    "data = pd.DataFrame({'area':area, 'pop':pop})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual ``Series`` that make up the columns of the ``DataFrame`` can be accessed via dictionary-style indexing of the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can use attribute-style access with column names that are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the ``Series`` objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case adding a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['density'] = data['pop'] / data['area']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select only a subset of columns from a `DataFrame`, we can use a list of column names instead of a single column name. In this example, a list containing `area` and `density` are used as an index parameter to the `DataFrame`, i.e. `['area', 'density']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['area', 'density']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame as Two-dimensional Array\n",
    "Secondly, consider the ``DataFrame`` as an enhanced two-dimensional array. We can examine the raw underlying data array using the ``values`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to indexing of ``DataFrame`` objects, however, it is clear that the dictionary-style indexing of columns precludes our ability to simply treat it as a NumPy array.\n",
    "In particular, passing a single index to an array accesses a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and passing a single \"index\" to a ``DataFrame`` accesses a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'><u>Worksheet Exercises</u></font>\n",
    "1. There is a file in the `data` folder called `california_cities.csv`. Read the contents of this file into a `DataFrame` called `cities`.\n",
    "2. Each column in `cities` can be represented as a `Series` object. Select only the `elevation_m` column from `cities`.\n",
    "3. Now, create a new `DataFrame` called `cities_metric` with the following columns from the original data: `city`, `elevation_m`, `area_land_km2`, `area_water_km2` and `pop_density`.\n",
    "4. Add a new column to `cities_metric` which shows the population density (i.e. population per square kilometer)\n",
    "5. Using masking, select only those cities in `cities_metric` whose elevation is greater than 1000m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your exercise solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous. In particular, many interesting datasets will have some amount of data missing. To make matters even more complicated, different data sources may indicate missing data in different ways. In this section, we will discuss some general considerations for missing data, discuss how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for handling missing data in Python. We refer to missing data in general as *null*, *NaN*, or *NA* values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data in Pandas\n",
    "\n",
    "The way in which Pandas handles missing values is constrained by its reliance on the NumPy package, which does not have a built-in notion of NA values for non-floating-point data types. Pandas uses sentinels (special values that indicate something) for missing data based on two pre-existing Python null values: the special floating-point ``NaN`` value, and the Python ``None`` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``None``: Pythonic missing data\n",
    "\n",
    "The first sentinel value used by Pandas is ``None``, a Python object that is often used for missing data in Python code. Because it is a Python object, ``None`` cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type ``'object'`` (i.e., arrays of Python objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4])\n",
    "vals1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``dtype=object`` means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects. While this kind of object array is useful for some purposes, any operations on the data will be done at the Python level, with much more overhead than the typically fast operations seen for arrays with native types. The use of Python objects in an array also means that if you perform aggregations like ``sum()`` or ``min()`` across an array with a ``None`` value, you will generally get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reflects the fact that addition between an integer and ``None`` is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``NaN``: Missing numerical data\n",
    "\n",
    "The other missing data representation, ``NaN`` (acronym for *Not a Number*), is different; it is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = np.array([1, np.nan, 3, 4]) \n",
    "vals2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NumPy chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into compiled code. NOTE: regardless of the operation, the result of arithmetic with ``NaN`` will be another ``NaN`` - this means that aggregates over the values are well defined (i.e., they don't result in an error) but not always useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.sum(), vals2.min(), vals2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy does provide some special aggregations that will ignore these missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that ``NaN`` is specifically a floating-point value; there is no equivalent NaN value for integers, strings, or other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN and None in Pandas\n",
    "\n",
    "``NaN`` and ``None`` both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, np.nan, 2, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For types that don't have an available sentinel value, Pandas automatically type-casts when NA values are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating on Null Values\n",
    "\n",
    "As we have seen, Pandas treats ``None`` and ``NaN`` as essentially interchangeable for indicating missing or null values.\n",
    "To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures.\n",
    "They are:\n",
    "\n",
    "- ``isnull()``: Generate a boolean mask indicating missing values\n",
    "- ``notnull()``: Opposite of ``isnull()``\n",
    "- ``dropna()``: Return a filtered version of the data\n",
    "- ``fillna()``: Return a copy of the data with missing values filled or imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting null values\n",
    "Pandas data structures have two useful methods for detecting null data: ``isnull()`` and ``notnull()``.\n",
    "Either one will return a Boolean mask over the data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean masks can be used directly as a ``Series`` or ``DataFrame`` index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``isnull()`` and ``notnull()`` methods produce similar Boolean results for ``DataFrame``s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping null values\n",
    "\n",
    "In addition to the masking used before, there are the convenience methods, ``dropna()``\n",
    "(which removes NA values) and ``fillna()`` (which fills in NA values). For a ``Series``,\n",
    "the result is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a ``DataFrame``, there are more options.\n",
    "Consider the following ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,      np.nan, 2],\n",
    "                   [2,      3,      5],\n",
    "                   [np.nan, 4,      6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot drop single values from a ``DataFrame``; we can only drop full rows or full columns.\n",
    "Depending on the application, you might want one or the other, so ``dropna()`` gives a number of options for a ``DataFrame``. By default, ``dropna()`` will drop all rows in which *any* null value is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can drop NA values along a different axis; ``axis='columns'`` drops all columns containing a null value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling null values\n",
    "\n",
    "Sometimes rather than dropping NA values, you'd rather replace them with a valid value.\n",
    "This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values.\n",
    "You could do this in-place using the ``isnull()`` method as a mask, but because it is such a common operation Pandas provides the ``fillna()`` method, which returns a copy of the array with the null values replaced.\n",
    "\n",
    "Consider the following ``Series``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill NA entries with a single value, such as zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a forward-fill to propagate the previous value forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward-fill\n",
    "data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can specify a back-fill to propagate the next values backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-fill\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For ``DataFrame``s, the options are similar, but we can also specify an ``axis`` along which the fills take place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if a previous value is not available during a forward fill, the NA value remains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'><u>Worksheet Exercises</u></font>\n",
    "1. The dataset (`california_cities.csv`) used throughout the last set of exercises includes some missing values. If this is not still in memory, read the contents of the file into a new `DataFrame` called `cities`.\n",
    "2. The missing values in this dataset are indicated by `NaN`. Filter out any rows that contain this value.\n",
    "3. Instead of filtering out rows with missing values, set the missing values to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your exercise solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets: Concat and Append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatenation of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. ``Series`` and ``DataFrame``s are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward.\n",
    "\n",
    "It will be useful for the next section to be able to show multiple dataframes at the same time. Here, we create a quick class that allows us to display multiple ``DataFrame``s side by side. An explanation of this programming construct is beyond the scope of this Jupyter Notebook (and the module). However, if you want to find out more about creating classes in Python see https://docs.python.org/3/tutorial/classes.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Concatenation with ``concat()``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a function, ``concat()``, which has a similar syntax to NumPy's ``concatenate()`` function. ``concat()`` can be used for a simple concatenation of ``Series`` or ``DataFrame`` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works to concatenate higher-dimensional objects, such as ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['A1','B1'],['A2','B2']],columns=['A', 'B'],index=[1, 2])\n",
    "df2 = pd.DataFrame([['A3','B3'],['A4','B4']],columns=['A', 'B'],index=[3, 4])\n",
    "\n",
    "# here we are using the class we created above to show these data frames side by side\n",
    "display('df1', 'df2', 'pd.concat([df1, df2])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the concatenation takes place row-wise within the ``DataFrame`` (i.e., ``axis='rows'``). ``concat()`` also allows specification of an axis along which concatenation will take place. The following example uses columns (``axis=\"columns\"``) to concatenate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame([['A0','B0'],['A1','B1']],columns=['A', 'B'],index=[0, 1])\n",
    "df4 = pd.DataFrame([['C0','D0'],['C1','D1']],columns=['C', 'D'],index=[0, 1])\n",
    "\n",
    "display('df3', 'df4', 'pd.concat([df3, df4], axis=\"columns\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation with joins\n",
    "\n",
    "So far, we have concatenated ``DataFrame``s with shared column names. In practice, data from different sources might have different sets of column names, and ``concat()`` offers several options in this case. Consider the concatenation of the following two ``DataFrame``s, which have some (but not all!) columns in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame([['A1','B1','C1'],['A2','B2','C2']],columns=['A', 'B', 'C'],index=[1, 2])\n",
    "df6 = pd.DataFrame([['B3','C3','D3'],['B4','C4','D4']],columns=['B', 'C', 'D'],index=[3, 4])\n",
    "\n",
    "display('df5', 'df6', 'pd.concat([df5, df6])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the ``join`` parameter of the concatenate function. By default, the join is a union of the input columns (``join='outer'``), but we can change this to an intersection of the columns using ``join='inner'``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df5', 'df6', 'pd.concat([df5, df6],join=\"inner\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ``append()`` method\n",
    "\n",
    "Because direct array concatenation is so common, ``Series`` and ``DataFrame`` objects have an ``append()`` method that can accomplish the same thing in fewer keystrokes.\n",
    "For example, rather than calling ``pd.concat([df1, df2])``, you can simply call ``df1.append(df2)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1', 'df2', 'df1.append(df2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'><u>Worksheet Exercises</u></font>\n",
    "1. In the first set of worksheet exercises, two `DataFrames` were created to record the percentage frequency of online purchases in 2020 for different age groups and gender. Combine these DataFrames to form a more complete picture of online purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your exercise solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets: Merge and Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the ``merge()`` function. The behavior implemented in ``merge()`` is a subset of what is known as *relational algebra*, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases. Pandas implements several of the fundamental relational algebra primitive operations in the ``merge()`` function and the related ``join()`` method of ``Series`` and ``Dataframe``s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories of Joins\n",
    "\n",
    "The ``merge()`` function implements a number of types of joins: the *one-to-one*, *many-to-one*, and *many-to-many* joins.\n",
    "All three types of joins are accessed via an identical call to the ``merge()`` interface; the type of join performed depends on the form of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-to-one joins\n",
    "\n",
    "The simplest type of merge expresion is the one-to-one join. It is very similar to the column-wise concatenation seen above. As a concrete example, consider the following two ``DataFrames`` which contain information on several employees in a company. To combine this information into a single ``DataFrame``, we can use the ``merge()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "df3 = pd.merge(df1, df2)\n",
    "\n",
    "display('df1', 'df2', 'pd.merge(df1, df2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``merge()`` function recognizes that each ``DataFrame`` has an `employee` column, and automatically joins using this column as a key. The result of the merge is a new ``DataFrame`` that combines the information from the two inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-one joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-one joins are joins in which one of the two key columns contains duplicate entries.\n",
    "For the many-to-one case, the resulting ``DataFrame`` will preserve those duplicate entries as appropriate.\n",
    "Consider the following example of a many-to-one join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "\n",
    "display('df3', 'df4', 'pd.merge(df3, df4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting ``DataFrame`` has an aditional column with the `supervisor` information, where the information is repeated in one or more locations as required by the inputs (e.g. `Guido` appears twice in row 1 and 2 because both `Jake` and `Lisa` are in the `Engineering` group)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-many joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right `DataFrame` contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example. Consider the following, where we have a ``DataFrame`` showing one or more skills associated with a particular group.\n",
    "By performing a many-to-many join, we can recover the skills associated with any individual person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "\n",
    "display('df1', 'df5', 'pd.merge(df1, df5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Merge Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen the default behavior of ``merge()``: it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and ``merge()`` provides a variety of options for handling this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ``on`` keyword\n",
    "\n",
    "Most simply, you can explicitly specify the name of the key column using the ``on`` keyword, which takes a column name or a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option works only if <u>both</u> the left and right ``DataFrame``s have the specified column name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ``left_on`` and ``right_on`` keywords\n",
    "\n",
    "At times you may wish to merge two datasets with different column names; for example, we may have a dataset in which the employee name is labeled as `name` rather than `employee`. In this case, we can use the ``left_on`` and ``right_on`` keywords to specify the two column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "\n",
    "display('df1', 'df3', 'pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has a redundant column that we can drop if desired–for example, by using the ``drop()`` method of ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop('name', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Set Arithmetic for Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join.\n",
    "This comes up when a value appears in one key column but not the other. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                   columns=['name', 'food'])\n",
    "df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "\n",
    "display('df6', 'df7', 'pd.merge(df6, df7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have merged two datasets that have only a single `name` entry in common: Mary. By default, the result contains the *intersection* of the two sets of inputs; this is what is known as an *inner join*. We can specify this explicitly using the ``how`` keyword, which defaults to ``'inner'``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options for the ``how`` keyword are ``'outer'``, ``'left'``, and ``'right'``.\n",
    "\n",
    "#### `outer` join\n",
    "An *outer join* returns a join over the union of the input columns, and fills in all missing values with NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='outer')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `left` join\n",
    "The *left join* and *right join* return joins over the left entries and right entries, respectively.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='left')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output rows now correspond to the entries in the left input. \n",
    "\n",
    "#### `right` join\n",
    "The *right join* return joins over the right entries. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='right')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'><u>Worksheet Exercises</u></font>\n",
    "There are three datasets in the `data` folder: `state-abbrevs.csv`, `state-areas.csv` and `state-populations.csv`.\n",
    "1. Use the `merge()` function, combine the `state-abbrevs.csv` and `state-areas.csv` datasets into a single `DataFrame` called `states`.\n",
    "2. Now, combine `states` with the dataset `state-populations.csv`. Use the same `DataFrame` name for the result, i.e. `states`.\n",
    "3. Now, drop the `state/region` duplicate column from `states` using `drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your exercise solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An essential piece of analysis of large data is efficient summarization: computing aggregations like ``sum()``, ``mean()``, ``median()``, ``min()``, and ``max()``, in which a single number gives insight into the nature of a potentially large dataset.\n",
    "In this section, we'll explore aggregations in Pandas, from simple operations akin to what we've seen on NumPy arrays, to more sophisticated operations based on the concept of a ``groupby``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planets Data\n",
    "\n",
    "Throughout this section, we will use a Planets dataset to provide a realistic set of data to aggregate. The dataset gives information on planets that astronomers have discovered around other stars (known as *extrasolar planets* or *exoplanets* for short). It includes some details on the 1,000+ extrasolar planets discovered up to 2014. We can use Pandas `read_csv()` function to read the CSV data into a ``DataFrame``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "planets = pd.read_csv('data\\planets.csv')\n",
    "\n",
    "planets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Aggregation in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Unit 6, we explored some of the data aggregations available for NumPy arrays. As with a one-dimensional NumPy array, for a Pandas ``Series``, the aggregates return a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.rand(5))\n",
    "\n",
    "print(ser.sum())\n",
    "print(ser.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a ``DataFrame``, by default the aggregates return results within each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': rng.rand(5),\n",
    "                   'B': rng.rand(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying the ``axis`` argument, you can instead aggregate within each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas ``Series`` and ``DataFrame``s include all of the common aggregates available to NumPy; in addition, there is a convenience method ``describe()`` that computes several common aggregates for each column and returns the result. (NOTE: essentially, this is what you are doing in the Programming Task 1 of the assignment). For the Planets data, we will drop any rows with missing values before we use the `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planets.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes some other built-in Pandas aggregations (applicable to both ``DataFrame`` and ``Series`` objects):\n",
    "\n",
    "| Aggregation              | Description                     |\n",
    "|--------------------------|---------------------------------|\n",
    "| ``count()``              | Total number of items           |\n",
    "| ``first()``, ``last()``  | First and last item             |\n",
    "| ``mean()``, ``median()`` | Mean and median                 |\n",
    "| ``min()``, ``max()``     | Minimum and maximum             |\n",
    "| ``std()``, ``var()``     | Standard deviation and variance |\n",
    "| ``mad()``                | Mean absolute deviation         |\n",
    "| ``prod()``               | Product of all items            |\n",
    "| ``sum()``                | Sum of all items                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go deeper into the data, however, simple aggregates are often not enough.\n",
    "The next level of data summarization is the ``groupby`` operation, which allows you to quickly and efficiently compute aggregates on subsets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy: Split, Apply, Combine\n",
    "\n",
    "Simple aggregations can give us a flavor of a dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the ``groupby`` operation. The name \"group by\" comes from a command in the SQL database language. Consider the ``groupby`` operation as a three step process: split, apply and combine:\n",
    "\n",
    "- The *split* step involves breaking up and grouping a ``DataFrame`` depending on the value of the specified key.\n",
    "- The *apply* step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.\n",
    "- The *combine* step merges the results of these operations into an output array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/split-apply-combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple example\n",
    "As a simple example of the ``groupby`` operation, let's take a look at using Pandas to perform the computation shown in the diagram above. We'll start by creating the input ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)}, columns=['key', 'data'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic split-apply-combine operation can be computed by passing the name of the desired key column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that what is returned is not a set of ``DataFrame``s, but a ``DataFrameGroupBy`` object. To produce a result, we can apply an aggregate to this ``DataFrameGroupBy`` object, which will perform the appropriate apply/combine steps to produce the desired result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``sum()`` method is just one possibility here; you can apply virtually any common Pandas or NumPy aggregation function, as well as virtually any valid ``DataFrame`` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'><u>Worksheet Exercises</u></font>\n",
    "For this exercise the `DataFrame` resulting from the previous set of exercises is required, i.e. `states`.\n",
    "1. Using the `groupby()` and `sum()` functions, find the sums of each numeric column for each state\n",
    "2. This output is not very informative. The dataset provides populations for the years between 1990 and 2013 in each state. Using another `groupby()` function, find the total population of the USA in each of these years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your exercise solutions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized String Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of *vectorized string operations* that become an essential piece of the type of munging required when cleaning up real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas String Operations\n",
    "Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the ``str`` attribute of Pandas ``Series`` and ``Index`` objects that contain strings. So, for example, we can call a single method that will capitalize all the entries, while skipping over any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.Series(['peter', 'Paul', None, 'MARY', 'gUIDO'])\n",
    "names.str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables of Pandas String Methods\n",
    "Nearly all Python's built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas ``str`` methods that mirror Python string methods:\n",
    "\n",
    "|             |                  |                  |                  |\n",
    "|-------------|------------------|------------------|------------------|\n",
    "|``len()``    | ``lower()``      | ``translate()``  | ``islower()``    | \n",
    "|``ljust()``  | ``upper()``      | ``startswith()`` | ``isupper()``    | \n",
    "|``rjust()``  | ``find()``       | ``endswith()``   | ``isnumeric()``  | \n",
    "|``center()`` | ``rfind()``      | ``isalnum()``    | ``isdecimal()``  | \n",
    "|``zfill()``  | ``index()``      | ``isalpha()``    | ``split()``      | \n",
    "|``strip()``  | ``rindex()``     | ``isdigit()``    | ``rsplit()``     | \n",
    "|``rstrip()`` | ``capitalize()`` | ``isspace()``    | ``partition()``  | \n",
    "|``lstrip()`` |  ``swapcase()``  |  ``istitle()``   | ``rpartition()`` |\n",
    "\n",
    "Here are some examples of these methods in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\n",
    "                   'Eric Idle', 'Terry Jones', 'Michael Palin'])\n",
    "monte.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the length of the strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to find which string begin with a `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.startswith('T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even to split the strings into separate words and return these as a ``Series`` of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods using regular expressions\n",
    "\n",
    "In addition, there are several methods that accept regular expressions to examine the content of each string element, and follow some of the API conventions of Python's built-in ``re`` module (see https://docs.python.org/3/library/re.html for more information) :\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| ``match()`` | Call ``re.match()`` on each element, returning a boolean. |\n",
    "| ``extract()`` | Call ``re.match()`` on each element, returning matched groups as strings.|\n",
    "| ``findall()`` | Call ``re.findall()`` on each element |\n",
    "| ``replace()`` | Replace occurrences of pattern with some other string|\n",
    "| ``contains()`` | Call ``re.search()`` on each element, returning a boolean |\n",
    "| ``count()`` | Count occurrences of pattern|\n",
    "| ``split()``   | Equivalent to ``str.split()``, but accepts regexps |\n",
    "| ``rsplit()`` | Equivalent to ``str.rsplit()``, but accepts regexps |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, you can do a wide range of interesting operations.\n",
    "For example, we can extract the first name from each by asking for a contiguous group of characters at the beginning of each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.extract('([A-Za-z]+)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do something more complicated, like finding all names that start and end with a consonant, making use of the start-of-string (``^``) and end-of-string (``$``) regular expression characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.findall(r'^[^AEIOU].*[^aeiou]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to concisely apply regular expressions across ``Series`` or ``Dataframe`` entries opens up many possibilities for analysis and cleaning of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous methods\n",
    "Finally, there are some miscellaneous methods that enable other convenient operations:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| ``get()`` | Index each element |\n",
    "| ``slice()`` | Slice each element|\n",
    "| ``slice_replace()`` | Replace slice in each element with passed value|\n",
    "| ``cat()``      | Concatenate strings|\n",
    "| ``repeat()`` | Repeat values |\n",
    "| ``normalize()`` | Return Unicode form of string |\n",
    "| ``pad()`` | Add whitespace to left, right, or both sides of strings|\n",
    "| ``wrap()`` | Split long strings into lines with length less than a given width|\n",
    "| ``join()`` | Join strings in each element of the Series with passed separator|\n",
    "| ``get_dummies()`` | extract dummy variables as a dataframe |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and time-indexed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and Times in Python\n",
    "\n",
    "The Python world has a number of available representations of dates, times, deltas, and timespans.\n",
    "While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Native Python dates and times: ``datetime`` and ``dateutil``\n",
    "\n",
    "Python's basic objects for working with dates and times reside in the built-in ``datetime`` module.\n",
    "Along with the third-party ``dateutil`` module, you can use it to quickly perform a host of useful functionalities on dates and times.\n",
    "For example, you can manually build a date using the ``datetime`` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime(year=2015, month=7, day=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using the ``dateutil`` module, you can parse dates from a variety of string formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "date = parser.parse(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of ``datetime`` and ``dateutil`` lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typed arrays of times: NumPy's ``datetime64``\n",
    "\n",
    "The weaknesses of Python's datetime format inspired the NumPy team to add a set of native time series data type to NumPy.\n",
    "The ``datetime64`` dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly.\n",
    "The ``datetime64`` requires a very specific input format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "date = np.array('2015-07-04', dtype=np.datetime64)\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this date formatted, however, we can quickly do vectorized operations on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + np.arange(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the uniform type in NumPy ``datetime64`` arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python's ``datetime`` objects, especially as arrays get large. One detail of the ``datetime64`` and ``timedelta64`` objects is that they are built on a *fundamental time unit*. Because the ``datetime64`` object is limited to 64-bit precision, the range of encodable times is $2^{64}$ times this fundamental unit. In other words, ``datetime64`` imposes a trade-off between *time resolution* and *maximum time span*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table lists all of the available fundamental time unit format codes (see https://numpy.org/doc/stable/reference/arrays.datetime.html for more information) :\n",
    "\n",
    "|Code    | Meaning     | \n",
    "|--------|-------------|\n",
    "| ``Y``  | Year\t       |\n",
    "| ``M``  | Month       |\n",
    "| ``W``  | Week\t       |\n",
    "| ``D``  | Day         |\n",
    "| ``h``  | Hour        |\n",
    "| ``m``  | Minute      |\n",
    "| ``s``  | Second      |\n",
    "| ``ms`` | Millisecond |\n",
    "| ``us`` | Microsecond |\n",
    "| ``ns`` | Nanosecond  |\n",
    "| ``ps`` | Picosecond  |\n",
    "| ``fs`` | Femtosecond |\n",
    "| ``as`` | Attosecond  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and times in Pandas: best of both worlds\n",
    "\n",
    "Pandas provides a ``Timestamp`` object, which combines the ease-of-use of ``datetime`` and ``dateutil`` with the efficient storage and vectorized interface of ``numpy.datetime64``. From a group of these ``Timestamp`` objects, Pandas can construct a ``DatetimeIndex`` that can be used to index data in a ``Series`` or ``DataFrame``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Time Series: Indexing by Time\n",
    "\n",
    "Where the Pandas time series tools really become useful is when you begin to *index data by timestamps*.\n",
    "For example, we can construct a ``Series`` object that has time indexed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04'])\n",
    "data = pd.Series([0, 1, 2, 3], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this data in a ``Series``, we can make use of any of the ``Series`` indexing patterns we discussed in previous sections, passing values that can be coerced into dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2014-07-04':'2015-07-04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional special date-only indexing operations, such as passing a year to obtain a slice of all data from that year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2015']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
